-   TCP (Transmission Control Protocol)

    What TCP guarantees

    * Connection-oriented: established via a handshake before data transfer.
    * Reliable delivery: lost packets are retransmitted.
    * Ordered delivery: receiver presents bytes in original send order.
    * Error-checking: checksums detect corruption.
    * Flow control: receiver advertises window (rwnd) to avoid overwhelming its buffers.
    * Congestion control: sender adapts sending rate to avoid congesting the network.

    Three-way handshake (establish)

    1. Client → Server: `SYN` (client seq = x)
    2. Server → Client: `SYN+ACK` (server seq = y, ack = x+1)
    3. Client → Server: `ACK` (ack = y+1)

    Why it matters:

    * Ensures both sides agree on initial sequence numbers.
    * Prevents old duplicate segments from being interpreted as new.

    Connection tear-down

    * Four-step `FIN`/`ACK` (half-close) or `RST` for abrupt termination.

    Flow control

    * Advertised window (rwnd): receiver tells sender how many bytes it can accept.
    * Prevents buffer overflow on slow receivers.

    Congestion control (core concepts)

    Key variables: cwnd (congestion window at sender), ssthresh.

    * Slow-start: cwnd begins small (1-10 MSS) and grows exponentially every RTT (cwnd += MSS per ACK), doubling per RTT until it hits `ssthresh`.
    * Congestion avoidance: after `ssthresh`, cwnd grows linearly (e.g., +MSS²/cwnd per ACK or cwnd += MSS*(MSS/cwnd) ).
    * Fast retransmit: upon 3 duplicate ACKs, a segment is presumed lost → retransmit without waiting for timeout.
    * Fast recovery: sometimes reduce `ssthresh` and set `cwnd = ssthresh + 3*MSS` then continue cautiously.

    Modern variants: CUBIC (Linux), BBR (Google) — aim to maximize throughput while minimizing queueing delay.

    Retransmission timeouts

    * RTO is computed from RTT and RTT variance (Jacobson/Karels).
    * Conservative to avoid spurious retransmits.

    Timers & keep-alive

    * Keep-alive: detect dead peer; configurable interval.
    * TIME-WAIT: after close, socket remains for 2×MSL (max segment lifetime) to avoid confusion with delayed packets.

    TCP-level performance tips

    * Use TCP keep-alive only when necessary.
    * Nagle’s algorithm coalesces small writes into larger segments — good to reduce packets but can increase latency for interactive apps; can be disabled (TCP_NODELAY).
    * SACK (Selective Acknowledgement) reduces retransmit costs for multiple lost segments.
    * Window scaling (RFC 7323) required to use >64 KB windows.
    * Tuning: buffer sizes, congestion control algorithm, sndbuf/rwnd, MSS.

    When to use TCP

    * Need reliable in-order delivery (web pages, file transfers, databases).
    * Protocols with heavy state or where retransmit semantics help.

-   UDP (User Datagram Protocol)

    UDP semantics

    * Connectionless, datagram-oriented.
    * Each datagram is independent; may be lost, reordered, duplicated.
    * No retransmissions, no ordering, no flow/congestion control (on protocol level).

    Advantages

    * Low latency and lower CPU overhead.
    * Useful for real-time apps that can tolerate loss (VoIP, real-time video, online gaming).
    * Allows application-level control over reliability semantics (FEC, selective retransmission).
    * Supports multicast and broadcast.

    Use cases

    * Media streaming (RTP/RTCP often over UDP)
    * DNS (UDP for lightweight queries, TCP fallback for large responses)
    * DHCP (broadcasts)
    * Real-time protocols (QUIC uses UDP as substrate)
    * Custom transport for high-frequency trading with application-level reliability

    Multicast / broadcast

    * UDP supports multicast groups (efficient distribution to multiple receivers).
    * Useful for live video distribution and discovery protocols (mDNS, SSDP).

    Pitfalls / considerations

    * NAT traversal: UDP may be blocked or require STUN/TURN for traversal.
    * Must implement congestion control at app layer (or use libraries that do).
    * Packet size: avoid fragmentation (keep < MTU, typically 1500 bytes).
    * No built-in security — often combined with DTLS (Datagram TLS) for encryption.

-   HTTP (Hypertext Transfer Protocol)

    Core model

    * Stateless, request/response, application-layer protocol.
    * Runs over TCP (HTTP/3 runs over QUIC/UDP).
    * Each request has method, URI, headers, optional body.

    HTTP methods (semantics)

    * GET: safe (no side effects), idempotent, cacheable.
    * HEAD: same as GET but response has no body.
    * POST: create/trigger; not idempotent by default.
    * PUT: create/replace resource; idempotent.
    * PATCH: partial update; not necessarily idempotent (depends).
    * DELETE: delete resource; idempotent (deleting same resource repeatedly has same effect).
    * OPTIONS / TRACE / CONNECT — other control methods.

    Idempotent: multiple identical requests produce same effect as one.

    Status code classes

    * 1xx (Informational)
    * 2xx (Success): e.g., 200 OK, 201 Created, 204 No Content
    * 3xx (Redirection): e.g., 301, 302, 304 Not Modified
    * 4xx (Client error): e.g., 400 Bad Request, 401 Unauthorized, 403 Forbidden, 404 Not Found, 409 Conflict, 429 Too Many Requests
    * 5xx (Server error): e.g., 500 Internal Server Error, 502 Bad Gateway, 503 Service Unavailable, 504 Gateway Timeout

    Common headers (important ones)

    * Host (mandatory in HTTP/1.1)
    * Content-Type / Content-Length
    * Transfer-Encoding: chunked
    * Connection: keep-alive / close
    * Cache-Control (no-cache, no-store, max-age, public/private)
    * Expires (older caching header)
    * ETag / If-None-Match (validation)
    * Last-Modified / If-Modified-Since
    * Vary (what headers affect cache)
    * Authorization (Bearer token)
    * WWW-Authenticate
    * Accept / Accept-Encoding / Accept-Language / Content-Encoding
    * CORS headers: `Origin`, `Access-Control-Allow-Origin`, etc.

    HTTP caching model (very important)

    * Freshness controlled by `Cache-Control: max-age`, `Expires`.
    * Validation: `ETag` or `Last-Modified` allow conditional `GET` with `If-None-Match` / `If-Modified-Since` → server returns `304 Not Modified` to indicate cached copy is valid.
    * Cache-Control directives: `public`, `private`, `no-cache`, `no-store`, `must-revalidate`, `s-maxage`.
    * `Vary` header indicates which request headers (e.g., `Accept-Encoding`, `User-Agent`) the cache key should vary on.

    Idempotency & safe methods

    * Safe methods (GET/HEAD) should not change server state.
    * Idempotent methods can be repeated safely (PUT, DELETE — though semantics depend).

    Connection semantics & performance

    * HTTP/1.0: usually one connection per request unless keep-alive is used.
    * HTTP/1.1: persistent connections by default (Connection: keep-alive).
    * Pipelining: send multiple requests without waiting for responses — not widely used because head-of-line (HOL) blocking and proxies issues.
    * Connection pooling on client side reduces cost of connection setup (esp. with TLS).

    Chunked transfer & streaming

    * `Transfer-Encoding: chunked` allows streaming responses without knowing content-length up front.
    * Useful for server-sent events, streaming logs, media.

    Compression

    * `Content-Encoding: gzip, br, deflate` reduces payload size — negotiated via `Accept-Encoding`.
    * Beware CPU cost for compressing many small responses.

-   HTTP versions: evolution & implications

    HTTP/1.1

    * Persistent connections
    * Pipelining (rarely used)
    * Head-of-line blocking at TCP level: a lost packet stalls all pipelined requests on that connection
    * Textual framing, no binary framing

    HTTP/2 (major improvements)

    * Binary framing
    * Multiplexing: many request/response streams over single TCP connection — eliminates application-level pipelining and reduces connection overhead
    * Header compression (HPACK)
    * Server push: server may push resources proactively (cautious use)
    * Priorities: client can set stream priorities
    * Still uses TCP → still susceptible to TCP-level HOL blocking: lost packet stalls all streams on same TCP connection until retransmitted.

    HTTP/3 + QUIC

    * QUIC is a protocol over UDP developed by Google (IETF standardizing), combining transport and TLS into a single protocol.
    * Per-stream multiplexing without TCP HOL: each QUIC stream is independent; loss affects only that stream.
    * Built-in TLS 1.3 for encryption; handshake faster (0-RTT for repeated connections).
    * QUIC has built-in congestion control (CUBIC, BBR variants).
    * HTTP/3 uses QUIC as transport, removing TCP-level HOL blocking and improving performance on lossy networks.

-   TLS (Transport Layer Security)

    Why TLS

    * Encrypts data in transit, provides confidentiality, integrity, and optionally authentication of endpoints.
    * Immutable best practice for public services.

    TLS handshake (high-level)

    * ClientHello (cipher suites, supported versions, SNI)
    * ServerHello (chosen suite), Certificate, ServerKeyExchange (if needed), ServerHelloDone
    * ClientKeyExchange, then both compute shared secrets, send `Finished` messages.
    * TLS 1.3 consolidates and reduces round trips: handshake more efficient; 0-RTT available for repeat clients (trade-off with replay protection).

    Certificates & PKI

    * Public Key Infrastructure: certificate chains from CA (trusted root)
    * Use SNI (Server Name Indication) to host multiple certs on same IP.
    * Certificate rotation: plan renewal automation (e.g., Let’s Encrypt).

    Session resumption

    * Session IDs or session tickets to avoid full handshake for repeat connections; reduces latency.

    TLS performance tips

    * Use TLS 1.3 where possible.
    * Enable HTTP/2 over TLS for multiplexing benefits.
    * Offload TLS at load balancer or use hardware accelerators when needed (but trust boundaries must be considered).
    * OCSP stapling to improve revocation checks without latency.

-   RPC (Remote Procedure Call)

    Purpose

    * Make remote procedure invocation look like local function calls for programmers.
    * Often used for internal service-to-service calls for performance and type-safety.

    RPC architecture (stubs)

    1. Client stub: Marshal args into a request.
    2. Transport: Send to server (HTTP/TCP).
    3. Server stub: Unmarshal, call actual function, marshal response.
    4. Client receives response and unmarshals result.

    Common RPC frameworks

    * gRPC (Protobuf + HTTP/2): efficient binary encoding, streaming, deadlines, metadata, built-in streaming, client/server streaming.
    * Thrift (Facebook/Apache): binary protocols, service definitions, multi-language.
    * Avro (Apache): used often with Kafka.
    * JSON-RPC / XML-RPC: text-based, simple.

    gRPC specifics

    * Uses Protobuf IDL for service + message definitions.
    * HTTP/2-based: multiplexed, streaming, efficient header compression.
    * Supports unary RPC and streaming (client, server, bidi).

    Sync vs Async RPC

    * Synchronous: client waits for response (blocking or async-await).
    * Asynchronous: client submits request and receives response later (or via callback/future).

    Strengths

    * Compact binary encoding → lower latency & bandwidth.
    * Strongly typed contracts (IDL) → compile-time checks.
    * Streaming easily supported (HTTP/2 streaming semantics used by gRPC).

    Weaknesses / risks

    * Tight coupling between client and server types/contract.
    * Harder to route through HTTP caches (not cache-friendly).
    * Complexity in debugging (binary encoding).
    * Cross-language tooling required.

    Reliability concerns & patterns

    * Idempotency: make RPC handlers idempotent where possible to tolerate retries.
    * Retries: use exponential backoff + jitter; careful with non-idempotent calls.
    * Deadlines/Timeouts: propagate deadlines from client to server to avoid wasted work.
    * Circuit breakers: stop calling unhealthy downstream services.
    * Bulkheads: isolate resource usage per caller/service.

-   REST (Representational State Transfer)

    REST constraints (Fielding)

    1. Client-server separation of concerns.
    2. Statelessness: each request contains all info to process (no server-side session state).
    3. Cacheable: responses labelled as cacheable/uncacheable.
    4. Uniform interface: resources identified via URIs; standard verbs used.
    5. Layered system: intermediaries (proxies, gateways) can exist.
    6. Optional: Code on demand (rare) — server can send executable code.

    HATEOAS (Hypermedia as the Engine of Application State)

    * Provide links in responses so clients discover actions (self-descriptive).
    * Rarely used in many practical APIs due to complexity but core to REST purists.

    Resource modeling best practices

    * Model nouns (resources), not RPC verbs.
    * Use nested resources for hierarchy: `GET /users/123/orders`.
    * For complex queries, use query parameters or dedicated search endpoints (`/orders?since=...&status=shipped`).

    Versioning strategies

    * URI versioning: `/v1/users/` (simple, explicit).
    * Header versioning: `Accept: application/vnd.myapp.v2+json`.
    * Query param: `?v=2` (less preferred).
    * Backwards compatibility: prefer additive changes; maintain multiple versions if needed.

    Caching & REST

    * Use `Cache-Control`, `ETag`, `Last-Modified`, `Expires`.
    * `Cache-Control: public` for CDN-enabled caches; `private` for user-specific content.

    Pagination

    * Use `limit`/`offset` for small datasets or cursor-based pagination for large/streaming sets.
    * Cursor-based (cursor/next_token) avoids skip/offset costs and is robust under inserts.

    Partial responses / Field selection

    * Allow clients to request subset of fields: `GET /users/123?fields=name,email` to reduce payload.

    Rate limiting & throttling

    * Return `429 Too Many Requests` with `Retry-After` header.
    * API keys/clients used to enforce quotas.

    Statelessness tradeoffs

    * Stateless scaling easier (no sticky sessions).
    * But some interactions (transactions, multi-step flows) require state management (store in client, or use tokens).

    REST pitfalls (mentioned in your content)

    * Hard to represent complex queries as simple URIs.
    * Multiple round trips to assemble a complex view (N+1 problem).
    * Response bloat over time (added fields).
    * Use GraphQL or custom aggregator endpoints if clients require flexible shapes.

-   REST vs RPC — Detailed Comparison

    Coupling

    * REST: looser coupling (resource-based). Good for public APIs.
    * RPC: tighter coupling (operation-based). Good for internal services.

    Performance / Efficiency

    * RPC: binary encoding (Protobuf) over HTTP/2 → efficient and low overhead.
    * REST: text-based (JSON) over HTTP/1.1/2 → larger but human-readable and cache-friendly.

    Caching / intermediaries

    * REST works naturally with HTTP caching/CDNs.
    * RPC often bypasses HTTP caches — not ideal for public caching.

    Evolution & versioning

    * REST: easier to evolve with hypermedia and field selection.
    * RPC: contract-first; must coordinate changes across clients.

    When to use what (practical)

    * REST for public-facing APIs, mobile clients, and browser clients (standard HTTP features, caching, visibility).
    * RPC / gRPC for internal microservice communication where performance & strict typing matter.

-   Sockets, Ports, NAT, Firewalls & Connection pooling

    Sockets & ports

    * Endpoint = IP + port.
    * Well-known ports: 80 (HTTP), 443 (HTTPS), 53 (DNS/UDP/TCP), etc.

    NAT (Network Address Translation)

    * NAT rewrites private internal IPs to a public IP.
    * Impacts inbound connectivity (servers need port forwarding or public IPs), complicates UDP (stateful NAT).

    Firewalls

    * Stateful firewalls track connections (e.g., allow return traffic for allowed outbound connections).
    * Configure to allow necessary ports only; e.g., 80/443 for web traffic.

    Connection pooling

    * Avoid expensive TCP/TLS handshakes per request — use connection pooling (keep-alive).
    * In microservices, use HTTP/2 multiplexing to reduce number of connections.
    * Database drivers often use connection pools to avoid creating new DB TCP connections for each request.

    Keep-alive & Timeouts

    * Keep-alive keeps TCP/TLS sessions open for multiple requests.
    * Idle keep-alive too long can exhaust file descriptors; tune `keepalive_timeout` and connection pool sizes.

    Pipelining vs multiplexing

    * HTTP/1.1 pipelining (rare) allows multiple requests without waiting for responses — has issues; disabled in many clients.
    * HTTP/2 multiplexing allows concurrent streams over single connection — preferred.

-   CORS, content negotiation, compression, streaming

    CORS (Cross-Origin Resource Sharing)

    * Browser security model: cross-origin requests blocked unless server allows via CORS headers.
    * Server should respond with `Access-Control-Allow-Origin` (and possibly `Access-Control-Allow-Methods`, `Access-Control-Allow-Headers`) to allow cross-origin requests.
    * Preflight `OPTIONS` request used for complex cross-origin calls.

    Content negotiation

    * Clients signal `Accept` header; server chooses representation (`application/json`, `application/xml`, `text/html`, etc.) or uses `Accept-Language`.

    Compression

    * Use gzip / brotli for payloads; negotiate via `Accept-Encoding`.
    * Trade-off: CPU for network bandwidth. Use thresholds to compress only larger payloads.

    Streaming & chunked transfer

    * Server-sent events, long-polling, WebSockets for real-time.
    * `Transfer-Encoding: chunked` for streaming large responses.

-    Security & Authentication

    Authentication

    * Basic (not recommended without TLS).
    * Bearer tokens (JWT) — stateless but must handle revocation.
    * OAuth 2.0 — authorization framework for delegated access (best practice for third-party access).
    * mTLS (mutual TLS) — both client & server authenticate via certs — great for internal services.
    * API keys — simple but limited; combine with rate limiting.

    Authorization

    * Enforce least privilege.
    * Implement RBAC / ABAC where needed.
    * Validate user identity and resource ownership.

    Input validation & injection protection

    * Sanitize inputs; defend against SQL injection, header injection, XSS via proper escaping and frameworks.

    TLS best practices

    * Enforce TLS 1.2+ (prefer 1.3).
    * HSTS for browsers.
    * Strict cipher suites, certificate pinning where appropriate.
    * Automate certificate rotation.

    Rate limiting & throttling

    * Protect service from abuse: per-client or global limits.
    * Use token bucket or leaky bucket algorithms.
    * Return `429` with `Retry-After`.

-   Observability: debugging, tracing, monitoring

    Metrics

    * Request rates (RPS), latency (p50/p95/p99), error rates, connection counts.
    * Transport-level metrics: TCP retransmits, RTT, bandwidth.

    Logging

    * Structured logs with correlation IDs (trace-id).
    * Include request/response metadata (method, path, status, duration).

    Distributed tracing

    * Trace context: propagate trace-id through headers (W3C Trace Context or `X-Request-ID`).
    * Tooling: Zipkin, Jaeger, Datadog APM, OpenTelemetry.
    * Trace includes spans (client call, server processing, downstream RPCs) to debug latency sources.

    Debugging binary protocols

    * Use tools: `tcpdump`, `wireshark` for packet inspection (careful with encrypted traffic).
    * For gRPC: use gRPC interceptors for logging; use textual Protobuf debug tools.

-   Operational best practices (timeouts, retries, circuit breakers)

    Timeouts

    * Always set per-call timeouts (connect timeout, read timeout).
    * Use shorter client-side timeouts than server internal processing timeouts to avoid resource exhaustion.

    Retries

    * Retries on idempotent ops only (GET/PUT/DELETE) or when server signals retryable error (HTTP 503/429).
    * Use exponential backoff + jitter to avoid synchronized retries (thundering herd).

    Circuit Breaker

    * Detect downstream failures and short-circuit to fail fast.
    * States: CLOSED → normal; OPEN → reject calls; HALF-OPEN → test a few calls.

    Bulkheads

    * Isolate resources per client or call type (e.g., separate thread pools) to prevent one slow dependency from blocking others.

    Load balancing & health checks

    * External LB (HAProxy, Nginx, cloud LB) or service mesh (Envoy) for routing.
    * Health checks: `/health` or `/ready` endpoints (liveness vs readiness).
    * Avoid binary health checks that cause load spikes.

-   Quick decision guide — when to pick what

    Use HTTP/REST (JSON) when:

    * Public APIs consumed by browsers or mobile
    * You want caching at CDN & proxies
    * You want simplicity and wide tooling & debuggability
    * Loose coupling is important

    Use gRPC / RPC when:

    * Internal microservices with high throughput/low latency needs
    * Strong typing and contract-first approach desired
    * You want streaming and efficient binary encoding
    * You control both client & server implementations

    Use TCP when:

    * You need reliable ordered delivery (most web traffic, databases)

    Use UDP when:

    * Low latency with lossy tolerance (media, games)
    * Need multicast/broadcast (discovery, streaming within LAN)
    * Using QUIC as underlying transport for HTTP/3

    Use HTTP/2 or HTTP/3:

    * Multiplexing for high parallelism (HTTP/2) — use with TLS
    * HTTP/3 (QUIC) when connection establishment & loss-resilience matter (mobile, lossy links)

-   Example patterns & practical snippets

    Example: client-side retry pseudo-code (idempotent)

    ```python
    def call_service(req):
        attempt, delay = 0, 0.1
        while attempt < max_retries:
            try:
                resp = http.request(req, timeout=timeout)
                if resp.status in [200,201,204]:
                    return resp
                if resp.status in [429, 503]:
                    transient server busy — retry
                    time.sleep(delay + jitter())
                    delay *= 2
                else:
                    return resp
            except (TimeoutError, ConnectionError):
                time.sleep(delay + jitter())
                delay *= 2
            attempt += 1
        raise Exception("Retries exhausted")
    ```

    Example: gRPC deadlines

    * Pass a deadline per call; server checks context for cancellation to abort work early.

-   Summary & checklist for system design interviews

    When designing a communication model, address:

    * Transport: TCP vs UDP vs QUIC (HTTP/3)? Reason: reliability/latency.
    * Protocol: REST vs RPC? Reason: public vs internal, caching, coupling.
    * Versioning & evolution: strategy to evolve APIs.
    * Performance: keep-alive, multiplexing, compression.
    * Security: TLS, auth (OAuth/JWT/mTLS).
    * Resiliency: timeouts, retries, circuit breakers, bulkheads.
    * Observability: metrics, logging, distributed tracing.
    * Operational: connection pooling, NAT/firewall rules, load balancing, health checks.
    * Caching & CDN: TTL, ETag, validation.
    * Testing & monitoring: latency p95/p99, error budgets, resource usage.
