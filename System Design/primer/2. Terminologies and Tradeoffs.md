-   Reliability 
    -   Reliability: Reliability refers to the ability of a system or component to perform its intended functions under specified conditions for a specified period of time.
        -   Focus: Consistency, dependability, and failure prevention.
        -   Key Questions:
            -   Does the system stay running without crashing?
            -   Does the system produce correct results every time?
            -   How long can the system operate without failure (Mean Time Between Failures)?
        A reliable system is one you can count on not to fail unexpectedly.

-   Performance
    -   Performance: Performance refers to how quickly, efficiently, and responsively a system carries out its tasks in relation to the amount of resources used.
        -   Focus: Speed, capacity, efficiency, and responsiveness.
        -   Key Questions:
            -   How quickly does the website load a page?
            -   How many transactions per second can the database handle?
            -   How much memory or CPU does the system consume under load?
        A high-performance system is one that operates quickly and efficiently.
    
-   Diff b/w Reliability and Redundancy

    -   Reliability is a system's ability to perform its function correctly over a specified time, while redundancy is the use of backup components or systems to improve reliability. 
    -   Redundancy is a design method to achieve a high level of reliability by creating backup paths for a required function. 
    -   For example, a web server with redundant backup servers improves its reliability, ensuring it can continue to operate if the primary server fails. 

    A car that starts every morning without fail over its lifetime : Reliability
    Having a spare tire in a car. Having multiple data centers so one can take over if another fails (like Netflix) : Redundancy

-   Diff b/w Reliability and Performance
    A system can be extremely fast (high performance) but prone to frequent crashes (low reliability). Alternatively, it might be slow but run consistently for years without failure (high reliability, low performance). The ideal system design balances both needs.

-   Performance v/s Scalability in Distributed Systems

    -   Core Idea
        - Scalability = proportional growth in performance/capacity when resources are added.
        - Resources are added not only for performance but also for reliability (fault tolerance).
        - A truly scalable system should not degrade when redundancy is introduced.

    -   Why Scalability Is Hard? 
        1. Scalability Cannot Be an Afterthought
            - If not designed for scale:
                - Adding nodes won’t help.
                - Bottlenecks appear (e.g., single-threaded coordinator).
                - Algorithms degrade with data growth (e.g., \(O(n^2)\) queries).
            - Example: Table scan queries → fine at 1M rows, catastrophic at 1B rows.

        2. System Growth Introduces Heterogeneity
            - As clusters grow:
                - Hardware differs (newer CPUs, more RAM).
                - Network topology varies (different racks/regions).
                - Bandwidth differences emerge.
            - If algorithms assume uniformity:
                - System breaks under heterogeneity.
                - Faster machines underutilized.
            - Example: Uniform hash partitioning → hotspots when node capacities differ.

    -   Achieving Good Scalability

        A scalable architecture must:

        - ✔ Define growth axis (storage, throughput, latency, concurrency).  
        - ✔ Support redundancy without collapse (replicated DBs like Cassandra, CockroachDB).  
        - ✔ Handle heterogeneity (weighted load balancing, adaptive partitioning).  
        - ✔ Use the right tools for the right scaling problem:
            - Caching → reduce repeated work.  
            - Load balancing → distribute requests fairly.  
            - Replication → fault tolerance & availability.  
            - Sharding → horizontal data partitioning.  
            - Asynchronous processing → decouple workloads.  
            - Event-driven architecture → scale with demand.

    -   Visual Mental Model

        Think of scalability as axes of growth:

        | Axis            | Example Tool/Technique |
        |-----------------|-------------------------|
        | Storage         | Sharding, distributed file systems |
        | Throughput      | Load balancing, replication |
        | Latency         | Caching, async pipelines |
        | Concurrency     | Event-driven architecture, partitioned queues |

    -   One-Sentence Definition
        A scalable system grows in performance or capacity proportionally with added resources, even in the presence of failures and heterogeneous hardware, provided it is architected for scale from the beginning.
    
    -   Another way to look at performance vs scalability:
        -   If you have a performance problem, your system is slow for a single user.
        -   If you have a scalability problem, your system is fast for a single user but slow under heavy load.

-   Latency V/S Throughput
    -   Latency is the time required to perform some action or to produce some result. Latency is measured in units of time -- hours, minutes, seconds, nanoseconds or clock periods.

    -   Throughput is the number of such actions executed or results produced per unit of time. This is measured in units of whatever is being produced (cars, motorcycles, I/O samples, memory words, iterations) per unit of time. The term "memory bandwidth" is sometimes used to specify the throughput of memory systems.

    -   Generally, you should aim for maximal throughput with acceptable latency.

-   CAP Theorem – Consistence V/S Availability
    -   What Is a Partition?
        - Partition = network fails to deliver messages between nodes.
        - Messages are lost, not just delayed.
        - If messages eventually arrive → not a partition.
        - Total partition:
            - No messages delivered between two sets of nodes.
            - Common in real systems (e.g., single switch/router failure → full isolation).

    -   CAP Theorem
        In a distributed system, across a read/write pair, you can only guarantee two of three:
        -   Consistency (C): Every read returns the most recent write.
        -   Availability (A): Every non-failing node returns a response (no error/timeout).
        -   Partition Tolerance (P): The system continues to function despite message loss (network partitions).
    
    -   Why Partition Tolerance Can’t Be Avoided
        - Distributed systems run on networks → networks fail, Links drop, switches reboot, zones isolate.
        - You cannot build a distributed system without handling partitions.
        - Therefore: P is always required → the real choice is C vs A.

    -   The Two Possible System Behaviors
        1️⃣ CP — Consistency + Partition Tolerance
            - System waits for unreachable node.
            - Reads/writes may timeout or error.
            - Guarantees atomic & strict correctness.
            - Examples: Zookeeper, HBase, MongoDB (majority mode).
            - Use cases: Banking, financial transactions, ordering systems.

        2️⃣ AP — Availability + Partition Tolerance
            - Always returns a response (may be stale).
            - Accepts writes even when nodes can’t reach each other.
            - Data reconciles later when partition heals.
            - Examples: Cassandra, DynamoDB, Riak.
            - Use cases: Shopping carts, real-time feeds, high-availability services.

        -   Key Insight
            - Partitions happen outside your software. You cannot prevent them.
            - Your only choice is:
                - Wait for unreachable nodes → CP
                - Serve stale data → AP
            - Choosing the wrong side can break your system under load or failure.

-   Consistency Patterns in Distributed Systems

    When data is stored in multiple replicas, we must decide how to synchronize them so clients see a consistent view.
    -   CAP definition of consistency: Every read returns the most recent write (or an error).

    1. Weak Consistency
        - Guarantee: After a write, future reads *may or may not* see the update.
        - Behavior:  
            - No strict ordering.  
            - Best effort propagation.  
        - Use cases: Real-time systems where missing updates is acceptable.
        - Examples:  
            - Memcached  
            - VoIP, video calls, multiplayer games  
                *(lost audio/video packets aren’t replayed)*

    2. Eventual Consistency
        - Guarantee: After a write, all replicas will eventually converge to the latest value.
        - Behavior:  
            - Replication is asynchronous.  
            - Reads may temporarily return stale data.  
        - Use cases: Highly available, distributed systems.
        - Examples:  
            - DNS  
            - Email  
            - Dynamo-style NoSQL systems (Cassandra, Riak, DynamoDB)

    3. Strong Consistency
        - Guarantee: After a write, all reads return the latest committed value.
        - Behavior:  
            - Replicas are synchronized synchronously.  
            - Clients wait for confirmation before proceeding.  
        - Use cases: Systems requiring correctness, ordering, and transactions.
        - Examples:  
            - Traditional RDBMS  
            - Distributed file systems (when configured for strong consistency)

    -   Synchronous vs Asynchronous
        - Synchronous: “Wait for response before proceeding.”  
        - Asynchronous: “Fire and forget; continue without waiting.”

    -   Key Insight
        Consistency patterns are a tradeoff between latency, correctness, and availability:
        - Weak → fastest, least strict.  
        - Eventual → balances availability with eventual correctness.  
        - Strong → strict correctness, higher latency.

-   Availability Patterns in Distributed Systems
    High availability is achieved through two complementary patterns: fail-over and replication.

    -   Fail-over
        -   Active-Passive
            - Mechanism:  
                - Heartbeats sent between active and passive server.  
                - If heartbeat fails → passive takes over active’s IP and resumes service.  
            - Downtime:  
                - Hot standby: passive already running → minimal downtime.  
                - Cold standby: passive must start up → longer downtime.  
            - Traffic: Only active server handles traffic.  
            - Alias: *Master-slave failover.*

        -   Active-Active
            - Mechanism:  
                - Both servers handle traffic simultaneously.  
                - Load is spread between them.  
            - Routing:  
                - Public-facing → DNS must know both IPs.  
                - Internal-facing → application logic must know both servers.  
            - Alias: *Master-master failover.*

        -   Disadvantages of Fail-over
            - Requires more hardware and adds complexity.  
            - Risk of data loss if active fails before replication to passive.

    -   Replication
        - Master-slave replication: One master, multiple read-only slaves.  
        - Master-master replication: Multiple masters, all can accept writes.  
        - Ensures redundancy and improves read availability.

    -   Availability in Numbers

        Availability is measured in number of 9s:

        | Availability | Downtime per Year | Downtime per Month | Downtime per Week | Downtime per Day |
        |--------------|------------------|--------------------|-------------------|------------------|
        | 99.9% (three 9s) | 8h 45m 57s | 43m 49.7s | 10m 4.8s | 1m 26.4s |
        | 99.99% (four 9s) | 52m 35.7s | 4m 23s | 1m 5s | 8.6s |


    -   Availability in Sequence vs Parallel
        In Sequence
        - Formula:  Availability (Total) = Availability (Foo) * Availability (Bar)
        - Example: Two components at 99.9% → total = 99.8%.

        In Parallel
        - Formula:  Availability (Total) = 1 - (1 - Availability (Foo)) * (1 - Availability (Bar))
        - Example: Two components at 99.9% → total = 99.9999%.

    -   Key Insight
        - Fail-over ensures continuity when nodes fail.  
        - Replication ensures redundancy and faster recovery.  
        - Availability math shows why parallel redundancy dramatically improves uptime compared to sequential dependencies.

