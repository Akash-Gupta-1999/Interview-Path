-   Scalability
    Scalability is the ability of a system to handle increasing workload or demand by adding resources without compromising performance.
    -   Vertical Scaling : Increasing the capacity of a single machine (like adding more CPU, RAM, or storage) to improve system performance.
    -   Horizontal Scaling : Adding more machines or servers to a system so it can handle greater loads in parallel.

    -   Pro's and con's
        1.  HS requires load balancer for requests while VS not.
        2.  VS is single point of failure while HS is resilient
        3.  VS will be fast as there will be interprocess communication while HS will be bit slow due to network calls between servers (Remote Procedure Calls)
        4.  Data Consistency is Issue in HS while in VS is not
        5.  VS has a hardware limit to which we can increase while HS can be scaled to a larger extent.

-   Load Balancer
    -   What is Load Balancing?
        Load Balancing is the process of distributing incoming traffic or workloads across multiple backend servers so that:
        - No single server becomes overloaded  
        - System performance remains fast  
        - Failure of one server does not take down the system  
        - Horizontal scaling becomes possible  

    Architecture: Client → Load Balancer → Server Pool (S1, S2, S3...)

    -   Why Load Balancing Is Needed
        - High Availability → Routes traffic to healthy servers if one crashes.  
        - Scalability → Add more servers behind LB to handle more traffic.  
        - Performance Optimization → Even distribution lowers latency.  
        - Fault Isolation → Prevents a slow server from affecting the entire system.  
        - Zero-Downtime Deployments → Servers can be removed during deploys without downtime.  

    -   Types of Load Balancers
        1. Hardware Load Balancers  
        - Expensive physical devices (e.g., F5, Citrix ADC).  
        - High performance.  
        - Used in banks, telecom, government sectors.  

        2. Software Load Balancers  
        - Installed on general hardware.  
        - Examples: Nginx, HAProxy, Envoy, Traefik.  
        - Cheaper and flexible.  

        3. Cloud Load Balancers  
        - Managed services: AWS ALB/NLB, GCP Load Balancer, Azure Load Balancer.  
        - Auto-scaling + global distribution.  

    -   Load Balancer Levels (OSI)
        1. Layer 4 (Transport – TCP/UDP)  
        - Routes based on Source IP, Destination IP, Port.  
        - Faster but less intelligent.  
        - Example: AWS NLB, HAProxy TCP mode.  

        2. Layer 7 (Application – HTTP/HTTPS)  
        - Routes based on URL path, cookie, headers, query params, JWT/user ID, A/B testing rules.  
        - More intelligent but slightly slower.  
        - Example: AWS ALB, Nginx, Envoy.  

    -   Load Balancing Algorithms
        1. Round Robin → Requests distributed sequentially across servers.  
        2. Weighted Round Robin → Stronger servers get more requests.  
        3. Least Connections → New request goes to server with fewest active connections.  
        4. Least Response Time → Routes to fastest responding server.  
        5. IP Hash (Consistent Hashing) → Same client always goes to same server (session affinity).  
        6. Random → Randomly picks a server (effective at scale).  

    -   Health Checks
        Load balancers continuously check server health:
        - Ping check  
        - TCP check  
        - HTTP GET `/health`  
        - Application-level checks (login, DB, cache dependencies)  

        Healthy servers → receive traffic  
        Unhealthy servers → isolated  

    -   Session Affinity (Sticky Sessions)
        Problem: User logs in → next request goes to another server → session missing.  

        Solutions:
        1. Cookie-based stickiness (LB inserts cookie).  
        2. IP Hash (same IP → same server).  
        3. Centralized session storage (Redis, Memcached, Database).  

    -   Load Balancer Architecture
            Internet
                │
        ┌─────────────┐
        │Load Balancer│
        └─────────────┘
        /     |     \
     S1      S2      S3

    -   Deployment Types
        1. Single LB  
            - Clients → LB → Servers  
            - ❌ Single point of failure.  

        2. HA Mode (High Availability)  
            Clients → LB1 (active)
                      LB2 (passive backup)
            - If LB1 fails → LB2 takes over.  
            - Used with VRRP, Keepalived, or cloud LB.  

    -   Load Balancing in Microservices
        1. Client-Side LB → Client picks server (Netflix Ribbon, gRPC round robin).  
        2. Server-Side LB → Classic LB in front of server pool (Nginx, AWS NLB).  
        3. Service Mesh LB → Smart proxies (Envoy sidecars) handle balancing, retries, circuit breaking (Istio, Linkerd).  

    -   Global Load Balancing (GSLB)
        - Used for multi-region deployments.  
        - Distributes traffic between regions (US, EU, Asia).  
        - Techniques: GeoDNS, Anycast routing, CDN routing.  
        - Example: User in India → India DC; User in US → US DC.  

    -   Example – Netflix
        Netflix uses:
        - AWS Elastic Load Balancers  
        - Service discovery + client-side LB  
        - Regional traffic routing  
        - Intelligent fallback mechanisms  

    -   Advantages
        - No single point of failure  
        - Efficient resource usage  
        - Faster response times  
        - Handles traffic spikes  
        - Zero-downtime deployments  
        - Improves reliability & scalability  

    -   Disadvantages
        - More complex architecture  
        - Requires health checks + monitoring  
        - LB itself must be highly available  
        - Session handling issues if not designed properly  

    -   Load Balancing + Consistent Hashing
        - Prevents reshuffling when servers change.  
        - Example:  
            - Add new server → only a few clients remapped.  
            - Remove server → minimal disruptions.  
        - Used in: Nginx, Envoy, Redis Cluster, Cassandra ring.  
