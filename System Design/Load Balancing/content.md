-   Load Balancer
    A Load Balancer (LB) is a system that distributes incoming network or application traffic across multiple servers to ensure:
    -   High availability
    -   Reliability
    -   Scalability
    -   Faster response time
    -   Zero or minimal downtime
    It sits between clients and backend servers.

    -   Why Do We Need a Load Balancer?
    Without LB:
    -   One server handles everything â†’ overload â†’ crashes.
    -   No redundancy â†’ if server dies â†’ system down.
    -   Uneven distribution â†’ wasted CPU on some, overloaded others.
    With LB:
    -   Traffic spreads evenly
    -   Fault tolerance (if one server dies, others take over)
    -   Better performance
    -   Scalable architecture (add/remove servers easily)

    -   What a Load Balancer Does
        1. Distributes Client Requests: Evenly or intelligently routes incoming traffic to multiple servers.
        2. Health Checks: Continuously checks which servers are alive. If a server fails â†’ LB stops sending traffic to it.
        3. Failover / Redundancy: Automatic rerouting when a server crashes.
        4. SSL Termination: LB can decrypt HTTPS traffic â†’ reduces CPU load on servers.
        5. Caching (in some LBs): Stores responses & serves them quickly.
        6. Session Persistence: Ensures requests from the same user go to the same backend server (sticky sessions).

    -   Types of Load Balancers
    1. Hardware Load Balancers
        -   Physical devices (expensive, used in large enterprises).
        Example: F5 Big-IP, Citrix Netscaler
    2. Software Load Balancers
        -   Installed on servers.
        Example: Nginx, HAProxy, Envoy
    3. Cloud Load Balancers
        -   Fully managed by cloud providers.
        Examples: AWS ELB / ALB, Google Cloud Load Balancer, Azure Load Balancer

    -   Load Balancing Algorithms
    1. Round Robin: Sends requests one-by-one to each server in order.
    2. Weighted Round Robin: Servers with more capacity get more requests.
    3. Least Connections: Send traffic to the server with the fewest active connections.
    4. IP Hash: Same client IP â†’ same backend server (session affinity). (Consistent Hashing)
    5. Random Choice: Randomly choose any available server.

    -   Layer 4 vs Layer 7 Load Balancers
        -   Layer 4 (Transport Layer LB)
            -   Uses TCP/UDP info (no looking inside HTTP content).
            -   Very fast
            -   Works on ports & IPs
                Example: AWS NLB, HAProxy L4 mode
        -   Layer 7 (Application Layer LB)
            -   Understands HTTP/HTTPS.
            -   Can route by URL path, headers, cookies
            -   Example: /api â†’ API servers, /static â†’ CDN
            -   Can do authentication, cache, WAF, etc.
                Examples: AWS ALB, Nginx, Envoy, Cloudflare

-   Consistent Hashing
    Consistent hashing is a technique used in distributed systems to distribute keys/data across many servers while minimizing data movement when servers join or leave.

    -   Where Itâ€™s Used
        - Distributed caches (Redis Cluster, Memcached)  
        - Databases (Cassandra, DynamoDB)  
        - Load balancers  
        - Distributed file systems  

    1. Why Do We Need Consistent Hashing?

        âŒ Problem with Normal Hashing
            A common approach: server = hash(key) % N
            Where N = number of servers.  

        - If N changes (server added/removed), almost all keys get remapped.  
        - This causes massive cache misses or data reshuffling.  

        Example:
        - Current: `hash(key) % 3`  
        - Add new server â†’ `hash(key) % 4`  
        - â¡ 75% of keys move to new locations â†’ bad for performance  

    2. How Consistent Hashing Solves This ![alt text](image-1.png)
        - Servers â†’ mapped to a ring  
        - Keys â†’ mapped to the same ring  
        - Rule: A key always goes to the first server clockwise from its hash position  

           (hash circle)
        0 ----------------- 2^32-1
           |     |      |
         S1     S2     S3

        - When a server joins or leaves â†’ only local movement  
        - Only keys between two adjacent servers are moved, not the whole dataset  

    3. How It Works (Step-by-Step)

        (a) Create the Hash Ring  
        - Hash space: `0 â†’ 2^32 - 1`  
        - Hash servers: `hash("server1")`, `hash("server2")`, `hash("server3")`  
        - Place them on the circle  

        (b) Hash the Keys  
        - Hash keys (user_id, session_id, cache_key, etc.)  
        - Place them on the same ring  

        (c) Key Assignment Rule  
        - Key belongs to the first server clockwise from its position  
        - Example: Key K â†’ next clockwise server S1  

    4. What Happens When Servers Change?

        âœ” New server joins â†’ gets a position in the ring, only steals keys from the next server clockwise  
        âœ” Server fails â†’ only its keys are reassigned to the next server  

        â¡ Minimal data movement compared to modulo hashing  

    5. Virtual Nodes (VNodes) â€“ Important
        Real clusters rarely have equal load.  
        Solution: give each physical server multiple virtual points on the ring.  

        | Physical Server | Virtual Nodes |
        |-----------------|---------------|
        | S1              | S1A, S1B, S1C |
        | S2              | S2A, S2B, S2C |
        | S3              | S3A, S3B, S3C |

        Benefits:
        - Better load balancing  
        - Fault tolerance  
        - Even distribution of keys  

    ğŸ‘‰ Cassandra, DynamoDB, and Redis Cluster rely heavily on VNodes  

    6. Example â€“ Key Assignment
        Servers hash to:  
        - S1 â†’ 10  
        - S2 â†’ 40  
        - S3 â†’ 80  

        Keys:  
        - K1 â†’ 12  
        - K2 â†’ 45  
        - K3 â†’ 70  

        Clockwise assignment:  
        - K1 â†’ S2  
        - K2 â†’ S3  
        - K3 â†’ S3  

        If S2 fails â†’ keys that mapped to S2 now go to S3  

    7. Where Consistent Hashing Is Used
        âœ” Caching â†’ Memcached, Redis Cluster (reduces cache invalidation on scale)  
        âœ” Databases â†’ Cassandra, DynamoDB (decides which node stores each partition)  
        âœ” Load Balancers â†’ Distribute users across servers with minimal session migration  

    8. Pros & Cons

        âœ… Advantages
        - Minimal key movement during scaling  
        - Great for dynamic environments where nodes join/leave  
        - Better load balancing (with virtual nodes)  
        - Works well for distributed caches & databases  

        âš ï¸ Disadvantages
        - Needs virtual nodes to avoid imbalance  
        - More complex than simple modulo hashing  
        - Requires a ring structure and hashing strategy  

- Sharding 
    Sharding is the process of splitting a large database or dataset into smaller, independent pieces (shards) so that:
    - Data is distributed across multiple machines  
    - Each shard handles only part of the total load  
    - The system achieves horizontal scalability  

    ğŸ‘‰ Sharding = Horizontal Partitioning

    Example:
    - Users Aâ€“F â†’ Server 1  
    - Users Gâ€“M â†’ Server 2  
    - Users Nâ€“Z â†’ Server 3  

 -  Why Sharding Is Needed ?
    As data grows, a single database instance hits limits:

    - CPU overloaded  
    - RAM insufficient  
    - Queries slow  
    - Storage full  
    - Write throughput bottleneck  

    â¡ Sharding lets you add more machines instead of upgrading to bigger ones.

 -  How Sharding Works (Simple)
    - Data is divided based on a shard key (column/attribute used for partitioning).  
    - Each shard is a complete independent DB, not just a table slice.  

    Example shard keys:
    - `user_id`  
    - `customer_id`  
    - `region`  
    - `email prefix`  

 -  Types of Sharding

    1. Hash-Based Sharding

        shard = hash(user_id) % N

        - âœ” Even distribution  
        - âŒ Adding/removing shards = heavy data movement (fixed by consistent hashing)  

        Used in: Redis (some modes), MongoDB (default hash sharding)

    2. Range-Based Sharding

        Shard 1 â†’ user_id 1â€“10,000
        Shard 2 â†’ user_id 10,001â€“20,000
        Shard 3 â†’ user_id 20,001â€“30,000

        - âœ” Easy to query by range  
        - âŒ Hotspots possible (e.g., recent users all go to last shard)  

        Used in: MySQL sharded systems, Big OLTP systems

    3. Directory / Lookup Table Sharding

        User A â†’ Shard 1
        User B â†’ Shard 3
        User C â†’ Shard 2

        - âœ” Flexible  
        - âœ” Easy to add shards  
        - âŒ Directory must be highly available  

        Used in: Uber (trips data), Facebook (lookup-based)

    4. Geo-Sharding
        Split by geographical region:
        - Asia shard  
        - Europe shard  
        - US shard  

        - âœ” Low latency  
        - âœ” Data residency compliance  
        - âŒ Hard to read across regions  

        Used in: Google Spanner, AWS DynamoDB Global Tables

 -  Sharding in NoSQL vs SQL

    SQL:
    - Manual in MySQL/PostgreSQL clusters  
    - Tools: Vitess, Citus  

    NoSQL:
    - Cassandra â†’ consistent hashing + vnodes  
    - MongoDB â†’ shard key + router (mongos)  
    - DynamoDB â†’ automatic partitioning  

 âœ… Advantages of Sharding
    - Scales horizontally (add servers easily)  
    - Improves read/write throughput  
    - Reduces load per database  
    - Improves fault isolation  
    - Enables storage distribution  

 âš ï¸ Disadvantages of Sharding
    - Complex to maintain  
    - Cross-shard queries are slow  
    - Joins across shards are difficult  
    - Resharding (changing shard key) is painful  
    - Distributed transactions are expensive  

 ğŸ“– Sharding Example â€“ User Table

    Without sharding â†’ one DB:

        users(id, name, email, password)

        â¡ Grows to 500M users â†’ slow.

    With sharding:
        - Shard 1: users 0â€“100M  
        - Shard 2: users 100Mâ€“200M  
        - Shard 3: users 200Mâ€“300M  
        - Shard 4: users 300Mâ€“400M  
        - Shard 5: users 400Mâ€“500M  

    Each shard has its own CPU, RAM, storage.

 ğŸ” Sharding vs Partitioning

    | Concept       | Meaning                                |
    |---------------|----------------------------------------|
    | Partitioning  | Dividing data inside a single DB server |
    | Sharding      | Data partitioned across multiple machines |

    ğŸ‘‰ Sharding = Distributed Partitioning

 ğŸ”„ Sharding + Consistent Hashing
    - Sometimes used together  
    - Consistent hashing decides the shard  
    - Shards are placed on a ring  
    - Adding/removing shards only relocates few keys  

    Used in: Cassandra, Dynamo, Redis Cluster  


